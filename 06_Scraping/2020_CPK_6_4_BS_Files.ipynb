{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа повышения квалификации (научно-педагогических) работников НИУ ВШЭ\n",
    "\n",
    "# Python для исследователей\n",
    "\n",
    "*Автор: Татьяна Рогович, НИУ ВШЭ*  \n",
    "\n",
    "## Web-scraping: сохраняем файлы\n",
    "\n",
    "С помощью Python мы можем автоматически сохранять файлы. Например, на страничках сотрудников ВШЭ могут быть выложены резюме. \n",
    "\n",
    "https://www.hse.ru/org/persons/150045920\n",
    "\n",
    "Давайте попробуем собрать адреса персональных страниц сотрудников факультета компьютерных наук и у тех сотрудников, у кого на странице есть файл с резюме, скачаем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http://www.hse.ru/staff/abashidze', 'Абашидзе Хатуна Датоевна'), ('http://www.hse.ru/org/persons/348330900', 'Абовьян Наталья Михайловна'), ('http://www.hse.ru/org/persons/160951223', 'Аброскин Илья Дмитриевич'), ('http://www.hse.ru/org/persons/112929840', 'Авдеев Роман Сергеевич'), ('http://www.hse.ru/staff/avdoshin', 'Авдошин Сергей Михайлович'), ('http://www.hse.ru/org/persons/202722706', 'Аветисян Арутюн Ишханович'), ('http://www.hse.ru/org/persons/27235963', 'Агамирзян Игорь Рубенович'), ('http://www.hse.ru/org/persons/158485737', 'Айзенберг Антон Андреевич'), ('http://www.hse.ru/org/persons/224839518', 'Аксенов Сергей Андреевич'), ('http://www.hse.ru/org/persons/318659918', 'Аланов Айбек')]\n"
     ]
    }
   ],
   "source": [
    "link = 'https://cs.hse.ru/persons'\n",
    "soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "\n",
    "# находим класс, который отличает ссылки на персональные страницы сотрудников\n",
    "persons = soup.find_all('a', {\"class\":\"fa-person__name\"}) \n",
    "\n",
    "# реконструируем ссылки и схораним кортеж - ссылка + ФИО сотрудника (и избавимся от дубликатов)\n",
    "persons_links = [('http:' + a.get('href'), a.text.strip()) for a in set(persons)]\n",
    "persons_links = sorted(persons_links, key=lambda x: x[1])\n",
    "\n",
    "# в нашем примере будем работать с первыми десятью сотрудниками, \n",
    "# чтобы ускорить время исполнения кода\n",
    "first_ten = persons_links[:10]\n",
    "print(first_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем код, чтобы скачать резюме одного сотрудника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "person = first_ten[1]\n",
    "page = requests.get(person[0]).text\n",
    "print('Резюме' in page) # проверяем, есть ли на странице ссылка на резюме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link\" href=\"/data/2020/03/27/18380351003185/Abovyan_CV.pdf\" target=\"_blank\">Резюме</a>]\n",
      "/data/2020/03/27/18380351003185/Abovyan_CV.pdf\n",
      "Абовьян Наталья Михайловна\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ссылка на резюме есть, поэтому продолжим обработку страницы\n",
    "soup_link = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "# находим нужную ссылку по тексту\n",
    "print(soup_link.find_all('a', text='Резюме')) \n",
    "\n",
    "# достаем сам адрес ссылки на резюме\n",
    "print(soup_link.find_all('a', text='Резюме')[0].get('href')) \n",
    "\n",
    "# восстанавливаем полный адрес ссылки на резюме\n",
    "cv_link = 'https://www.hse.ru' + soup_link.find_all('a', text='Резюме')[0].get('href')\n",
    "\n",
    "# достаем имя сотрудника для того, чтобы назвать файл\n",
    "name = person[1]\n",
    "print(name)\n",
    "\n",
    "# обращаемся по ссылке к файлу\n",
    "doc = requests.get(cv_link)\n",
    "\n",
    "# проверяем расширение файла, который хранится по ссылке\n",
    "print(doc.headers['Content-Type'].split('/')[1] == 'pdf')\n",
    "\n",
    "\n",
    "# создаем файл в режиме записи бинарной информации\n",
    "with open(f'{name}.pdf', 'wb') as fh:\n",
    "    # записываем в файл содержание файла по ссылке в байтах\n",
    "    fh.write(doc.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте напишем функцию и применим ее ко всем ссылкам в нашем списке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CV(person):\n",
    "    page = requests.get(person[0]).text\n",
    "    if 'Резюме' in page:\n",
    "        soup_link = BeautifulSoup(page, 'lxml')\n",
    "        cv_link = 'https://www.hse.ru' + soup_link.find_all('a', text='Резюме')[0].get('href')\n",
    "        if doc.headers['Content-Type'].split('/')[1] == 'pdf':\n",
    "            name = person[1]\n",
    "            with open(f'{name}.pdf', 'wb') as fh:\n",
    "                fh.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in first_ten:\n",
    "    get_CV(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельное задание: \n",
    "На странице сотрудников есть информация о владении иностранными языками. Для сотрудников факультета создайте словарь, где ключом будет язык, а значением - количество сотрудников, которые указали, что владеют им. Выведите ключи и значения словаря, отсортированные по значениям от большего к меньшему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553\n"
     ]
    }
   ],
   "source": [
    "print(len(persons_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random \n",
    "\n",
    "lang_dict = {}\n",
    "cnt = 0\n",
    "\n",
    "for link in persons_links:\n",
    "    cnt += 1\n",
    "    if cnt % 50 == 0:\n",
    "        print(f'Обрабатываю ссылку №{cnt} из {len(persons_links)}')\n",
    "    try:\n",
    "        time.sleep(random.randint(1,5))\n",
    "        page = requests.get(link[0]).text\n",
    "        soup_link = BeautifulSoup(page, 'lxml')\n",
    "        lang = soup_link.find_all('dl', {'class':\"main-list large main-list-language-knowledge-level\"})[0]\n",
    "        for l in lang.find_all('dd'):\n",
    "            l_text = l.get_text()\n",
    "            lang_dict[l_text] = lang_dict.get(l_text, 0) + 1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(sorted(lang_dict.items(), key = lambda x: x[0]), end='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
