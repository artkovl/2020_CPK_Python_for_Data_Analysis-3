{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа повышения квалификации (научно-педагогических) работников НИУ ВШЭ\n",
    "\n",
    "# Python для исследователей\n",
    "\n",
    "*Татьяна Рогович, НИУ ВШЭ*\n",
    "\n",
    "## Bag of Words (мешок слов), лемматизация и классификация текстов\n",
    "\n",
    "Семинар\n",
    "\n",
    "*Автор: Татьяна Рогович, НИУ ВШЭ*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое NLP?\n",
    "Обработка естественного языка - (Natural Language Processing, NLP ), представляет собой набор методов для решения задач по обработке текстов. В этом уроке, мы попробуем загрузить и почистить текст новостей, а затем, использую простую модель признаков - мешка слов (Bag of Words), научимся довольно точно предсказывать фэйковая это новость или настоящая.\n",
    "\n",
    "Но для начала посмотрим как работает bag of words на игрушечном примере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем признаки (features) из Bag of Words\n",
    "\n",
    "Как превратить текст в некоторое численное представление для машинного обучения? Один из таких подходов называется мешком слов (Bag of Words). Модель Bag of Words изучает все слова, которые есть в нашем корпусе, а затем моделирует каждый документ, подсчитывая количество раз, когда появляется каждое слово. Например, рассмотрим следующие два предложения:\n",
    "\n",
    "Предложение 1: \"The cat sat on the hat\"\n",
    "\n",
    "Предложение 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "Для этих двух сообщений,получим следующий словарь:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Чтобы получить наш мешок слов, мы подсчитаем сколько раз каждое слово появилось в предложении. В предложение 1, \"the\" появилось два раза, а \"cat\", \"sat\", \"on\", и \"hat\" по одному разу каждый. Поэтому вектор признаков для этого предложения выглядит так:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "{ 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Аналогично, признаки предложения 2 будут такими: \n",
    "\n",
    "{ 3, 1, 0, 0, 1, 1, 1, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как перевести текст в признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если наша задача типовая, то скорее всего в sklearn уже есть библиотека, с помощью которой можно ее решить. Мы будем пользоваться классом CountVectorizer(), который при обучении создает признаки из всех слов обучающей выборки, при трансформации - подсчитывает встречание этих слов в тренировочной и тестовой выборке. А потом получившиеся наборы данных мы уже передаем алгоритмам для предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим тренировочный список текстов\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'call 222-22-22', 'please call me.. please']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем и иниициализируем класс CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучаемся на данных\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22', '222', 'cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Смотрим сгенерированные признаки\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в наш векторайзер уже зашита базовая обработка текста. Он приводит текст к нижнему регистру, и с помощью регулярок забирает только слова, состоящие из цифр и букв (знаки препинания удаляются), удаляет стоп-слова.\n",
    "\n",
    "Метод fit здесь разбивает текст на токены и сохраняет их в \"модель\". Метод transform мы используем, чтобы создать разреженную матрицу, в которой мы будем хранить информацию, сколько каждое слово встречалось в каждом \"тексте\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобразовываем исходный текст в матрицу признаков\n",
    "simple_train_matrix = vect.transform(simple_train)\n",
    "# каждый ряд - одно наблюдение (наш документ), каждая колонка - один признак (слово). \n",
    "# На пересечении - количество слов в документе.\n",
    "simple_train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [2, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sparse matrix*\n",
    "\n",
    "Разреженная матрица. Хранит только координаты ненулевых значений. Сильно экономит место. \n",
    "\n",
    "*dense matrix*\n",
    "\n",
    "Плотная матрица. Хранит все данные.\n",
    "Если у вас матрица 1000 на 1000 из нулей - она все это хранит (1 мб ничего)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>222</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call you tonight</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Call me a cab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call 222-22-22</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please call me.. please</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         22  222  cab  call  me  please  tonight  you\n",
       "call you tonight          0    0    0     1   0       0        1    1\n",
       "Call me a cab             0    0    1     1   1       0        0    0\n",
       "call 222-22-22            2    1    0     1   0       0        0    0\n",
       "please call me.. please   0    0    0     1   1       2        0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(simple_train_matrix.toarray(), columns=vect.get_feature_names(), index=simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 0)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t2\n",
      "dense matrix\n",
      "[[0 0 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 0]\n",
      " [2 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 1 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('sparse matrix')\n",
    "print(simple_train_matrix)\n",
    "\n",
    "print('dense matrix')\n",
    "print(simple_train_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим текст для теста и преобразуем обученным CountVectorizer\n",
    "simple_test = ['Please don\\'t call me, I will be busy']\n",
    "simple_test_matrix = vect.transform(simple_test)\n",
    "simple_test_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>222</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Please don't call me, I will be busy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      22  222  cab  call  me  please  tonight  \\\n",
       "Please don't call me, I will be busy   0    0    0     1   1       1        0   \n",
       "\n",
       "                                      you  \n",
       "Please don't call me, I will be busy    0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# куда делся don't?\n",
    "pd.DataFrame(simple_test_matrix.toarray(), columns=vect.get_feature_names(), index=simple_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform для тестовой выборки будет игнорировать токены, которые он не видел раньше (поэтому важен большой размер обучающей выборки и ее репрезентативность корпуса текстов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг (Stemming) и Лемматизация (Lemmatization)\n",
    "\n",
    "Процесс стемминга заключается в преобразовании различных вариантов одного и того же слова в одну ядерную форму. Например у нас есть слова \"running\", \"runs\" и \"run\", которые семантически говорят об одном и том же, поэтому мы можем вместо них использовать одно слово \"run\". Стоит отметить, что в этом случае мы можем потерять некоторые грамматические признаки, например форму времени.\n",
    "\n",
    "Воспользуемся алгоритмом Портера из модуля NLTK. Посмотрим на работу алгоритма на примере с вариантами слов run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rogov\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\rogov\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rogov\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of running is: run\n",
      "The stemmed form of runs is: run\n",
      "The stemmed form of run is: run\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С русским языком не работает, к сожалению. Но сам Портер адаптировал алгоритм позже для ряда языков (смотри ниже Snowball stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of бегать is: бегать\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of бегать is: {}\".format(stemmer.stem(\"бегать\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы успешно обрезали наши слова до базовой формы. Алгоритм не использует баз основ слов, а лишь, применяя последовательно ряд правил, отсекает окончания и суффиксы, основываясь на особенностях языка, в связи с чем работает быстро, но не всегда безошибочно.\n",
    "\n",
    "Однако у стемминга есть тенденция к \"грубому\" обрезанию концов слов. Например, для leaves мы получим следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of leaves is: leav\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одной стороны это нормально, но базовой формой слова будет leaf. Поэтому на помощь нам приходит лемматизация.\n",
    "\n",
    "Лемматизация пытается достигнуть того же эффекта, но в отличие от стеммера, лемматизация использует реальный словарь слов и поэтому не будет обрубать окончания слов, а будет возвращать лемму.\n",
    "\n",
    "Снова воспользуемся модулем NLTK и проверим, что он нам выдаст на leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized form of leaves is: leaf\n"
     ]
    }
   ],
   "source": [
    "# Импортируем\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Инициализируем \n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили верный ответ. Лемматизация обеспечивает нам более тонкую настройку по сравнению со стеммингом. Но чтобы интегрировать лемматизацию в count vectorizer, очистку текста придется делать сначала вручную (см. ниже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с русскоязычными текстами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С поддержкой русского языка у nltk не супер, к сожалению. Можно воспользоваться алгоритмом для стемминга, который является адаптацией стеммера Портера (но с русским языком ему сильно тяжелее, чем с английским)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежа\n",
      "бегущ\n",
      "бега\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "rus_stemmer = RussianStemmer()\n",
    "\n",
    "print(rus_stemmer.stem('бежать'))\n",
    "print(rus_stemmer.stem('бегущий'))\n",
    "print(rus_stemmer.stem('бегающий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если библиотека pymorphy, которая делает лемматизацию, основываясь на словаре проекта OpenCorpora. https://pymorphy2.readthedocs.io/\n",
    "\n",
    "Далее объяснения и примеры из документации.\n",
    "\n",
    "В pymorphy2 для морфологического анализа слов (русских) есть класс MorphAnalyzer.\n",
    "Экземпляры класса MorphAnalyzer занимают порядка 10-15Мб оперативной памяти (т.к. загружают в память словари, данные для предсказателя и т.д.); старайтесь ораганизовать свой код так, чтоб создавать экземпляр MorphAnalyzer заранее и работать с этим единственным экземпляром в дальнейшем.\n",
    "\n",
    "Метод MorphAnalyzer.parse() принимает слово (обязательно в нижнем регистре) и возвращает все возможные разборы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в этом примере слово “стали” может быть разобрано и как глагол (“они стали лучше справляться”), и как существительное (“кислородно-конверторный способ получения стали”). На основе одной лишь информации о том, как слово пишется, понять, какой разбор правильный, нельзя, поэтому анализатор может возвращать несколько вариантов разбора.\n",
    "\n",
    "**Выбор правильного разбора**  \n",
    "pymorphy2 возвращает все допустимые варианты разбора, но на практике обычно нужен только один вариант, правильный.\n",
    "\n",
    "У каждого разбора есть параметр score. Score - это оценка P(tag|word), оценка вероятности того, что данный разбор правильный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 умеет разбирать не только словарные слова; для несловарных слов автоматически задействуется предсказатель. Например, попробуем разобрать слово “бутявковедами” - pymorphy2 поймет, что это форма творительного падежа множественного числа существительного “бутявковед”, и что “бутявковед” - одушевленный и мужского рода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бутявковедами', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='бутявковед', score=1.0, methods_stack=((<FakeDictionary>, 'бутявковедами', 52, 10), (<KnownSuffixAnalyzer>, 'едами')))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('бутявковедами')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У каждого разбора есть нормальная форма, которую можно получить, обратившись к атрибуту normal_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'стать'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')[0]\n",
    "p.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 умеет склонять (ставить в какую-то другую форму) слова. Чтобы просклонять слово, его нужно сначала разобрать - понять, в какой форме оно стоит в настоящий момент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявка'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka = morph.parse('бутявки')[0]\n",
    "butyavka.normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бутявки'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.inflect({'gent'})[0] # родительный падеж"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью атрибута lexeme можно получить лексему слова:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бутявка', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явка', 8, 0), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 1), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявке', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явке', 8, 2), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявку', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явку', 8, 3), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкой', tag=OpencorporaTag('NOUN,inan,femn sing,ablt'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкой', 8, 4), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкою', tag=OpencorporaTag('NOUN,inan,femn sing,ablt,V-oy'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкою', 8, 5), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявке', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явке', 8, 6), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 7), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявок', tag=OpencorporaTag('NOUN,inan,femn plur,gent'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явок', 8, 8), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявкам', tag=OpencorporaTag('NOUN,inan,femn plur,datv'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явкам', 8, 9), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявки', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явки', 8, 10), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявками', tag=OpencorporaTag('NOUN,inan,femn plur,ablt'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явками', 8, 11), (<UnknownPrefixAnalyzer>, 'бут'))),\n",
       " Parse(word='бутявках', tag=OpencorporaTag('NOUN,inan,femn plur,loct'), normal_form='бутявка', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'явках', 8, 12), (<UnknownPrefixAnalyzer>, 'бут')))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butyavka.lexeme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теги и граммемы в pymorphy2 записываются латиницей (например, NOUN). Но часто удобнее использовать кириллические названия граммем (например, СУЩ вместо NOUN). Чтобы получить тег в виде строки, записанной кириллицей, используйте свойство OpencorporaTag.cyr_repr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),))\n",
      "VERB,perf,intr plur,past,indc\n",
      "ГЛ,сов,неперех мн,прош,изъяв\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(p.tag)\n",
    "print(p.tag.cyr_repr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем преобразовать предложение в список нормализованных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Лемматизация – это процесс преобразования слова в его базовую форму. Разница между стемминг (stemming) и лемматизацией заключается в том, что лемматизация учитывает контекст и преобразует слово в его значимую базовую форму, тогда как стемминг просто удаляет последние несколько символов, что часто приводит к неверному значению и орфографическим ошибкам.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_words = [word.lower() for word in re.findall(r'[а-яА-Я]+', test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_morph = [morph.parse(word)[0].normal_form for word in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лемматизация\n",
      "это\n",
      "процесс\n",
      "преобразования -> преобразование\n",
      "слова -> слово\n",
      "в\n",
      "его -> он\n",
      "базовую -> базовый\n",
      "форму -> форма\n",
      "разница\n",
      "между\n",
      "стемминг\n",
      "и\n",
      "лемматизацией -> лемматизация\n",
      "заключается -> заключаться\n",
      "в\n",
      "том -> тот\n",
      "что\n",
      "лемматизация\n",
      "учитывает -> учитывать\n",
      "контекст\n",
      "и\n",
      "преобразует -> преобразовать\n",
      "слово\n",
      "в\n",
      "его -> он\n",
      "значимую -> значимый\n",
      "базовую -> базовый\n",
      "форму -> форма\n",
      "тогда\n",
      "как\n",
      "стемминг\n",
      "просто\n",
      "удаляет -> удалять\n",
      "последние -> последний\n",
      "несколько\n",
      "символов -> символ\n",
      "что\n",
      "часто\n",
      "приводит -> приводить\n",
      "к\n",
      "неверному -> неверный\n",
      "значению -> значение\n",
      "и\n",
      "орфографическим -> орфографический\n",
      "ошибкам -> ошибка\n"
     ]
    }
   ],
   "source": [
    "for word, form in zip(test_words, test_morph):\n",
    "    if word != form:\n",
    "        print(word, '->', form)\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rogov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")  \n",
    "# Скачиваем нужные нам наборы. Необязательно скачивать все. Нам пока что нужен только\n",
    "# corpora stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать nltk для получения списка стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Импортируем список стол-слов\n",
    "print(stopwords.words(\"english\")) # Стоп-слова для английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"russian\")) # Стоп-слова для русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are those news fake or real?\n",
    "У нас есть датасет состоящий из заголовка новости, текста новости и лейбла, который показывает фейковая это новость или реальная.\n",
    "\n",
    "Нашей задачей будет натренировать модель, чтобы она могла определить фейковая новость или реальная."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/rogovich/Data/master/data/fake_or_real_news.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три основных столбца называются \"title\", \"text\", и \"label\".\n",
    "\n",
    "Разделим наши данные на обучающую и тестовую выборки, чтобы потом можно было проверить насколько качественно работает наш алгоритм. За y обозначим наш лейбл. За X саму колонку с текстом новостей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label']\n",
    "X = data[['text']] # двойные скобки, чтобы она осталась датафреймом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разделения данных на тестовую и обучающую выборки, воспользуемся функцией train_test_split из модуля sklearn. Как видно из названия она делит выборку на части и размещает данные по выбранным массивам. Test_size указывает на то, какую часть выборки нужно отложить для теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33,random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разбили данные и получили обучающую выборку, посмотрим как выглядит текст новости. Выведем любую новость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4244, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2091, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Copyright Violation Do you think there will be as many doom sayers if trump should get in office ? I notice here at GLP the amount of doom sayers seems to go down when a republican is in office (Bush). But when the left get in office the doomsaying increases. Now i am sure the effect is opposite. If trump gets in office i am sure the doomsaying will increase on the left side of the political spectrum. Page 1\n"
     ]
    }
   ],
   "source": [
    "print(X_train.iloc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда внутри текста мы можем иногда увидить HTML тэги (например \"br\"), аббревиатуры, пунктуацию, которые являются распространенными проблемы при обработке текста из Интернета. Но вроде в наших данных этого нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продолжим с новостями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее документацию CountVectorizer можно почитать тут:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем объект \"CountVectorizer\", метод для работы с bag of words\n",
    "# из scikit-learn\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             stop_words = 'english',   \\\n",
    "                             max_features = 1000) \n",
    "                                # max_features = ограничиваем максимальное\n",
    "                                # количество слов-признаков для ускорения\n",
    "                                # работы алгоритма. Если не ограничивать -\n",
    "                                # количество признаков будет равно всем\n",
    "                                # уникальным словам в нашем корпусе выборки\n",
    "\n",
    "# fit_transform() делает две вещи: Сначала он фитит модель\n",
    "# и изучает словарь; Потом трансформирует нашу обучающую выборку\n",
    "# в вектор признаков В fit_transform мы передаем список слов\n",
    "train_data_features = vectorizer.fit_transform(X_train['text'])\n",
    "\n",
    "# Используем удобные массивы из Numpy\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как выглядят наши обучающие данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4244, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 5, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ней 4244 строк и 1000 признаков (по одному на каждое слово в словаре).\n",
    "\n",
    "Теперь наша модель Bag of Words натренирована, посмотрим на получившийся словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '20', '2008', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '22', '24', '25', '26', '27', '28', '30', '40', '50', '60', 'ability', 'able', 'abortion', 'access', 'according', 'account', 'accused', 'act', 'action', 'actions', 'activists', 'actually', 'added', 'adding', 'address', 'administration', 'african', 'age', 'agency', 'agenda', 'agents', 'ago', 'agree', 'agreement', 'ahead', 'air', 'al', 'allies', 'allow', 'allowed', 'america', 'american', 'americans', 'announced', 'answer', 'anti', 'appear', 'appeared', 'appears', 'approach', 'april', 'arabia', 'area', 'areas', 'aren', 'armed', 'army', 'article', 'ask', 'asked', 'assad', 'associated', 'attack', 'attacks', 'attempt', 'attention', 'attorney', 'author', 'authorities', 'authority', 'available', 'average', 'away', 'backed', 'bad', 'ballot', 'bank', 'banks', 'barack', 'base', 'based', 'battle', 'began', 'begin', 'beginning', 'believe', 'believes', 'benefits', 'bernie', 'best', 'better', 'biden', 'big', 'biggest', 'billion', 'bit', 'black', 'board', 'body', 'boehner', 'book', 'border', 'born', 'brain', 'bring', 'british', 'brought', 'budget', 'build', 'building', 'bush', 'business', 'california', 'called', 'calling', 'calls', 'came', 'campaign', 'campaigns', 'candidate', 'candidates', 'capital', 'care', 'carolina', 'carson', 'case', 'cases', 'cause', 'center', 'central', 'century', 'certain', 'certainly', 'chairman', 'challenge', 'chance', 'change', 'changed', 'changes', 'check', 'chief', 'child', 'children', 'china', 'choice', 'chris', 'christie', 'church', 'cities', 'citizens', 'city', 'civil', 'claim', 'claimed', 'claims', 'class', 'classified', 'clear', 'clearly', 'climate', 'clinton', 'clintons', 'close', 'cnn', 'coalition', 'college', 'com', 'come', 'comes', 'comey', 'coming', 'comment', 'comments', 'committee', 'common', 'community', 'companies', 'company', 'completely', 'concerns', 'conference', 'conflict', 'congress', 'congressional', 'conservative', 'conservatives', 'consider', 'considered', 'constitution', 'continue', 'continued', 'continues', 'control', 'convention', 'corporate', 'corruption', 'cost', 'council', 'countries', 'country', 'county', 'course', 'court', 'cover', 'coverage', 'create', 'created', 'crime', 'criminal', 'crisis', 'critical', 'crowd', 'cruz', 'culture', 'current', 'currently', 'cut', 'daily', 'dangerous', 'data', 'david', 'day', 'days', 'dead', 'deal', 'death', 'debate', 'debt', 'decades', 'decided', 'decision', 'deep', 'defense', 'delegates', 'democracy', 'democrat', 'democratic', 'democrats', 'department', 'described', 'despite', 'details', 'development', 'did', 'didn', 'different', 'difficult', 'directly', 'director', 'district', 'documents', 'does', 'doesn', 'doing', 'dollars', 'don', 'donald', 'donors', 'doubt', 'drug', 'earlier', 'early', 'earth', 'east', 'easy', 'economic', 'economy', 'education', 'effect', 'effort', 'efforts', 'elected', 'election', 'elections', 'electoral', 'elite', 'email', 'emails', 'employees', 'end', 'energy', 'enforcement', 'entire', 'especially', 'establishment', 'europe', 'european', 'event', 'events', 'evidence', 'exactly', 'example', 'executive', 'expect', 'expected', 'experience', 'experts', 'face', 'facebook', 'fact', 'failed', 'fall', 'false', 'families', 'family', 'far', 'father', 'favor', 'fbi', 'fear', 'federal', 'feel', 'felt', 'field', 'fight', 'fighting', 'figure', 'final', 'finally', 'financial', 'florida', 'focus', 'follow', 'followed', 'following', 'food', 'force', 'forced', 'forces', 'foreign', 'form', 'forward', 'foundation', 'fox', 'france', 'free', 'freedom', 'french', 'friday', 'friends', 'funding', 'future', 'game', 'gave', 'gay', 'general', 'george', 'gets', 'getting', 'given', 'giving', 'global', 'god', 'goes', 'going', 'gold', 'gone', 'good', 'gop', 'got', 'gov', 'government', 'governor', 'great', 'greater', 'ground', 'group', 'groups', 'growing', 'growth', 'gun', 'guns', 'guy', 'half', 'hampshire', 'hand', 'hands', 'happen', 'happened', 'hard', 'having', 'head', 'health', 'hear', 'heard', 'heart', 'held', 'help', 'helped', 'high', 'higher', 'highly', 'hillary', 'history', 'hit', 'hold', 'home', 'hope', 'hour', 'hours', 'house', 'http', 'huge', 'human', 'hundreds', 'husband', 'idea', 'illegal', 'image', 'immediately', 'immigrants', 'immigration', 'impact', 'important', 'include', 'included', 'including', 'income', 'increase', 'increasingly', 'independent', 'individual', 'industry', 'influence', 'information', 'infowars', 'inside', 'instead', 'insurance', 'intelligence', 'interests', 'international', 'interview', 'investigation', 'involved', 'iowa', 'iran', 'iraq', 'iraqi', 'isis', 'islamic', 'isn', 'israel', 'israeli', 'issue', 'issues', 'james', 'january', 'jeb', 'job', 'jobs', 'john', 'johnson', 'judge', 'july', 'june', 'just', 'justice', 'kasich', 'key', 'killed', 'kind', 'king', 'knew', 'know', 'known', 'knows', 'labor', 'lack', 'land', 'large', 'largely', 'largest', 'late', 'later', 'latest', 'law', 'lawmakers', 'laws', 'lead', 'leader', 'leaders', 'leadership', 'leading', 'leave', 'led', 'left', 'legal', 'legislation', 'let', 'letter', 'level', 'liberal', 'libya', 'life', 'light', 'like', 'likely', 'line', 'list', 'little', 'live', 'lives', 'living', 'll', 'local', 'long', 'longer', 'look', 'looking', 'lose', 'lost', 'lot', 'love', 'low', 'lower', 'main', 'mainstream', 'major', 'majority', 'make', 'makes', 'making', 'man', 'march', 'marco', 'mark', 'market', 'marriage', 'mass', 'massive', 'matter', 'maybe', 'mean', 'means', 'media', 'medical', 'meet', 'meeting', 'member', 'members', 'men', 'message', 'met', 'mexico', 'michael', 'middle', 'military', 'million', 'millions', 'mind', 'minister', 'modern', 'moment', 'monday', 'money', 'month', 'months', 'morning', 'mother', 'movement', 'mr', 'muslim', 'muslims', 'nation', 'national', 'nations', 'nato', 'natural', 'nature', 'near', 'nearly', 'necessary', 'need', 'needed', 'needs', 'netanyahu', 'network', 'new', 'news', 'night', 'nomination', 'nominee', 'non', 'north', 'note', 'noted', 'november', 'nuclear', 'number', 'numbers', 'obama', 'obamacare', 'october', 'offered', 'office', 'officer', 'officers', 'official', 'officials', 'ohio', 'oil', 'old', 'ones', 'online', 'open', 'operation', 'operations', 'opinion', 'opportunity', 'opposition', 'order', 'organization', 'outside', 'page', 'paid', 'paper', 'parents', 'paris', 'particular', 'particularly', 'parties', 'party', 'pass', 'passed', 'past', 'path', 'paul', 'pay', 'peace', 'people', 'percent', 'period', 'person', 'personal', 'phone', 'place', 'plan', 'planned', 'plans', 'play', 'podesta', 'point', 'points', 'police', 'policies', 'policy', 'political', 'politicians', 'politics', 'poll', 'polling', 'polls', 'poor', 'popular', 'population', 'position', 'positions', 'possible', 'post', 'posted', 'potential', 'power', 'powerful', 'present', 'presidency', 'president', 'presidential', 'press', 'pressure', 'pretty', 'previous', 'primary', 'prime', 'prison', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'professor', 'program', 'project', 'protect', 'provide', 'public', 'published', 'push', 'putin', 'question', 'questions', 'quickly', 'quite', 'race', 'racial', 'racist', 'radio', 'raise', 'raised', 'rally', 'rate', 'reach', 'read', 'ready', 'reagan', 'real', 'reality', 'really', 'reason', 'reasons', 'received', 'recent', 'recently', 'record', 'records', 'red', 'reform', 'regime', 'region', 'related', 'relations', 'relationship', 'release', 'released', 'religious', 'remain', 'remains', 'remember', 'rep', 'report', 'reported', 'reporters', 'reports', 'republican', 'republicans', 'research', 'respect', 'response', 'rest', 'result', 'results', 'return', 'revealed', 'review', 'right', 'rights', 'rise', 'risk', 'role', 'romney', 'room', 'rubio', 'rule', 'rules', 'ruling', 'run', 'running', 'russia', 'russian', 'ryan', 'safe', 'said', 'sanders', 'saturday', 'saudi', 'saw', 'say', 'saying', 'says', 'school', 'science', 'second', 'secret', 'secretary', 'security', 'seen', 'self', 'sen', 'senate', 'senator', 'senior', 'sense', 'sent', 'september', 'series', 'serve', 'served', 'server', 'service', 'services', 'set', 'seven', 'sex', 'sexual', 'share', 'shooting', 'short', 'shot', 'showed', 'showing', 'shows', 'sign', 'significant', 'similar', 'simply', 'single', 'site', 'situation', 'small', 'social', 'society', 'son', 'soon', 'source', 'sources', 'south', 'speak', 'speaker', 'speaking', 'special', 'speech', 'spending', 'spent', 'spoke', 'spokesman', 'staff', 'stage', 'stand', 'standing', 'star', 'start', 'started', 'state', 'statement', 'states', 'status', 'stay', 'step', 'stop', 'stories', 'story', 'strategy', 'street', 'strong', 'students', 'study', 'sunday', 'super', 'support', 'supporters', 'supporting', 'supreme', 'sure', 'syria', 'syrian', 'taken', 'takes', 'taking', 'talk', 'talking', 'talks', 'target', 'tax', 'taxes', 'team', 'ted', 'tell', 'telling', 'term', 'terms', 'terror', 'terrorism', 'terrorist', 'terrorists', 'test', 'texas', 'thing', 'things', 'think', 'thinking', 'thought', 'thousands', 'threat', 'thursday', 'time', 'times', 'today', 'told', 'took', 'total', 'town', 'trade', 'tried', 'troops', 'true', 'trump', 'trust', 'truth', 'try', 'trying', 'tuesday', 'turkey', 'turn', 'turned', 'tv', 'twitter', 'ukraine', 'understand', 'union', 'united', 'university', 'usa', 'use', 'used', 'using', 'various', 've', 'veterans', 'vice', 'victory', 'video', 'view', 'views', 'violence', 'virginia', 'vote', 'voted', 'voter', 'voters', 'votes', 'voting', 'wall', 'want', 'wanted', 'wants', 'war', 'wars', 'washington', 'wasn', 'watch', 'water', 'way', 'ways', 'weapons', 'website', 'wednesday', 'week', 'weeks', 'went', 'west', 'western', 'white', 'wife', 'wikileaks', 'willing', 'win', 'wing', 'winning', 'wins', 'woman', 'women', 'won', 'word', 'words', 'work', 'worked', 'workers', 'working', 'world', 'worse', 'worth', 'wouldn', 'wrong', 'wrote', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно, можно вывести сколько раз слово встречается в словаре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1694 000\n",
      "1421 10\n",
      "548 100\n",
      "1006 11\n",
      "570 12\n",
      "357 13\n",
      "380 14\n",
      "762 15\n",
      "404 16\n",
      "363 17\n",
      "394 18\n",
      "779 20\n",
      "628 2008\n",
      "355 2010\n",
      "428 2011\n",
      "907 2012\n",
      "539 2013\n",
      "743 2014\n",
      "862 2015\n",
      "2681 2016\n",
      "344 22\n",
      "342 24\n",
      "557 25\n",
      "389 26\n",
      "384 27\n",
      "394 28\n",
      "708 30\n",
      "441 40\n",
      "564 50\n",
      "330 60\n",
      "424 ability\n",
      "859 able\n",
      "398 abortion\n",
      "598 access\n",
      "2089 according\n",
      "413 account\n",
      "345 accused\n",
      "1114 act\n",
      "826 action\n",
      "529 actions\n",
      "331 activists\n",
      "1161 actually\n",
      "854 added\n",
      "334 adding\n",
      "566 address\n",
      "1504 administration\n",
      "497 african\n",
      "394 age\n",
      "609 agency\n",
      "376 agenda\n",
      "392 agents\n",
      "1055 ago\n",
      "365 agree\n",
      "582 agreement\n",
      "485 ahead\n",
      "777 air\n",
      "957 al\n",
      "461 allies\n",
      "497 allow\n",
      "413 allowed\n",
      "2549 america\n",
      "3664 american\n",
      "2170 americans\n",
      "652 announced\n",
      "501 answer\n",
      "1079 anti\n",
      "384 appear\n",
      "421 appeared\n",
      "436 appears\n",
      "395 approach\n",
      "374 april\n",
      "373 arabia\n",
      "586 area\n",
      "408 areas\n",
      "377 aren\n",
      "375 armed\n",
      "511 army\n",
      "747 article\n",
      "471 ask\n",
      "1197 asked\n",
      "392 assad\n",
      "461 associated\n",
      "1257 attack\n",
      "1128 attacks\n",
      "358 attempt\n",
      "516 attention\n",
      "471 attorney\n",
      "453 author\n",
      "495 authorities\n",
      "360 authority\n",
      "364 available\n",
      "418 average\n",
      "1147 away\n",
      "369 backed\n",
      "711 bad\n",
      "360 ballot\n",
      "503 bank\n",
      "391 banks\n",
      "680 barack\n",
      "479 base\n",
      "999 based\n",
      "401 battle\n",
      "657 began\n",
      "330 begin\n",
      "362 beginning\n",
      "1416 believe\n",
      "342 believes\n",
      "335 benefits\n",
      "813 bernie\n",
      "1068 best\n",
      "1302 better\n",
      "492 biden\n",
      "1400 big\n",
      "411 biggest\n",
      "627 billion\n",
      "370 bit\n",
      "1709 black\n",
      "406 board\n",
      "465 body\n",
      "449 boehner\n",
      "560 book\n",
      "570 border\n",
      "383 born\n",
      "467 brain\n",
      "579 bring\n",
      "366 british\n",
      "452 brought\n",
      "548 budget\n",
      "462 build\n",
      "480 building\n",
      "2272 bush\n",
      "1036 business\n",
      "573 california\n",
      "1850 called\n",
      "515 calling\n",
      "477 calls\n",
      "1181 came\n",
      "5047 campaign\n",
      "355 campaigns\n",
      "2218 candidate\n",
      "1700 candidates\n",
      "345 capital\n",
      "1056 care\n",
      "550 carolina\n",
      "428 carson\n",
      "1590 case\n",
      "471 cases\n",
      "536 cause\n",
      "849 center\n",
      "509 central\n",
      "372 century\n",
      "462 certain\n",
      "477 certainly\n",
      "518 chairman\n",
      "443 challenge\n",
      "515 chance\n",
      "1699 change\n",
      "379 changed\n",
      "391 changes\n",
      "389 check\n",
      "754 chief\n",
      "454 child\n",
      "975 children\n",
      "821 china\n",
      "551 choice\n",
      "406 chris\n",
      "484 christie\n",
      "413 church\n",
      "368 cities\n",
      "624 citizens\n",
      "1264 city\n",
      "620 civil\n",
      "602 claim\n",
      "449 claimed\n",
      "513 claims\n",
      "809 class\n",
      "404 classified\n",
      "1204 clear\n",
      "395 clearly\n",
      "595 climate\n",
      "11659 clinton\n",
      "349 clintons\n",
      "864 close\n",
      "1180 cnn\n",
      "456 coalition\n",
      "592 college\n",
      "1283 com\n",
      "1627 come\n",
      "855 comes\n",
      "717 comey\n",
      "771 coming\n",
      "502 comment\n",
      "726 comments\n",
      "958 committee\n",
      "582 common\n",
      "823 community\n",
      "506 companies\n",
      "542 company\n",
      "336 completely\n",
      "423 concerns\n",
      "524 conference\n",
      "396 conflict\n",
      "1764 congress\n",
      "522 congressional\n",
      "1137 conservative\n",
      "491 conservatives\n",
      "452 consider\n",
      "394 considered\n",
      "419 constitution\n",
      "863 continue\n",
      "448 continued\n",
      "406 continues\n",
      "1220 control\n",
      "884 convention\n",
      "434 corporate\n",
      "353 corruption\n",
      "499 cost\n",
      "347 council\n",
      "968 countries\n",
      "3063 country\n",
      "432 county\n",
      "1018 course\n",
      "1789 court\n",
      "353 cover\n",
      "335 coverage\n",
      "517 create\n",
      "453 created\n",
      "539 crime\n",
      "542 criminal\n",
      "573 crisis\n",
      "331 critical\n",
      "400 crowd\n",
      "2010 cruz\n",
      "399 culture\n",
      "669 current\n",
      "386 currently\n",
      "442 cut\n",
      "502 daily\n",
      "376 dangerous\n",
      "792 data\n",
      "503 david\n",
      "2354 day\n",
      "1322 days\n",
      "452 dead\n",
      "1417 deal\n",
      "737 death\n",
      "1816 debate\n",
      "527 debt\n",
      "598 decades\n",
      "418 decided\n",
      "921 decision\n",
      "404 deep\n",
      "815 defense\n",
      "783 delegates\n",
      "471 democracy\n",
      "498 democrat\n",
      "2706 democratic\n",
      "1841 democrats\n",
      "1583 department\n",
      "464 described\n",
      "717 despite\n",
      "351 details\n",
      "329 development\n",
      "2715 did\n",
      "1330 didn\n",
      "1005 different\n",
      "470 difficult\n",
      "423 directly\n",
      "909 director\n",
      "351 district\n",
      "334 documents\n",
      "1715 does\n",
      "1201 doesn\n",
      "1061 doing\n",
      "430 dollars\n",
      "3001 don\n",
      "2701 donald\n",
      "412 donors\n",
      "329 doubt\n",
      "367 drug\n",
      "823 earlier\n",
      "1087 early\n",
      "440 earth\n",
      "691 east\n",
      "348 easy\n",
      "1095 economic\n",
      "835 economy\n",
      "512 education\n",
      "403 effect\n",
      "633 effort\n",
      "534 efforts\n",
      "627 elected\n",
      "3668 election\n",
      "773 elections\n",
      "434 electoral\n",
      "390 elite\n",
      "1323 email\n",
      "1343 emails\n",
      "334 employees\n",
      "1451 end\n",
      "611 energy\n",
      "528 enforcement\n",
      "617 entire\n",
      "652 especially\n",
      "834 establishment\n",
      "552 europe\n",
      "438 european\n",
      "610 event\n",
      "448 events\n",
      "959 evidence\n",
      "425 exactly\n",
      "713 example\n",
      "725 executive\n",
      "366 expect\n",
      "518 expected\n",
      "518 experience\n",
      "371 experts\n",
      "824 face\n",
      "758 facebook\n",
      "1551 fact\n",
      "488 failed\n",
      "431 fall\n",
      "342 false\n",
      "432 families\n",
      "1143 family\n",
      "1617 far\n",
      "422 father\n",
      "376 favor\n",
      "1592 fbi\n",
      "545 fear\n",
      "1694 federal\n",
      "696 feel\n",
      "372 felt\n",
      "548 field\n",
      "871 fight\n",
      "555 fighting\n",
      "412 figure\n",
      "524 final\n",
      "459 finally\n",
      "741 financial\n",
      "1019 florida\n",
      "508 focus\n",
      "533 follow\n",
      "337 followed\n",
      "698 following\n",
      "557 food\n",
      "1191 force\n",
      "369 forced\n",
      "1071 forces\n",
      "1629 foreign\n",
      "463 form\n",
      "513 forward\n",
      "763 foundation\n",
      "1101 fox\n",
      "367 france\n",
      "1201 free\n",
      "723 freedom\n",
      "403 french\n",
      "893 friday\n",
      "501 friends\n",
      "411 funding\n",
      "852 future\n",
      "403 game\n",
      "551 gave\n",
      "391 gay\n",
      "1460 general\n",
      "728 george\n",
      "450 gets\n",
      "745 getting\n",
      "1060 given\n",
      "404 giving\n",
      "893 global\n",
      "479 god\n",
      "476 goes\n",
      "3011 going\n",
      "563 gold\n",
      "370 gone\n",
      "1795 good\n",
      "1822 gop\n",
      "1139 got\n",
      "685 gov\n",
      "3878 government\n",
      "719 governor\n",
      "1210 great\n",
      "351 greater\n",
      "649 ground\n",
      "1665 group\n",
      "915 groups\n",
      "438 growing\n",
      "385 growth\n",
      "734 gun\n",
      "329 guns\n",
      "370 guy\n",
      "684 half\n",
      "628 hampshire\n",
      "630 hand\n",
      "408 hands\n",
      "517 happen\n",
      "567 happened\n",
      "960 hard\n",
      "886 having\n",
      "637 head\n",
      "1503 health\n",
      "353 hear\n",
      "446 heard\n",
      "405 heart\n",
      "690 held\n",
      "1334 help\n",
      "438 helped\n",
      "1258 high\n",
      "473 higher\n",
      "330 highly\n",
      "4744 hillary\n",
      "1158 history\n",
      "494 hit\n",
      "603 hold\n",
      "1107 home\n",
      "660 hope\n",
      "353 hour\n",
      "577 hours\n",
      "3227 house\n",
      "338 http\n",
      "390 huge\n",
      "910 human\n",
      "345 hundreds\n",
      "388 husband\n",
      "766 idea\n",
      "576 illegal\n",
      "363 image\n",
      "382 immediately\n",
      "547 immigrants\n",
      "965 immigration\n",
      "366 impact\n",
      "1117 important\n",
      "393 include\n",
      "380 included\n",
      "1729 including\n",
      "483 income\n",
      "500 increase\n",
      "369 increasingly\n",
      "472 independent\n",
      "356 individual\n",
      "472 industry\n",
      "490 influence\n",
      "1282 information\n",
      "391 infowars\n",
      "427 inside\n",
      "895 instead\n",
      "386 insurance\n",
      "742 intelligence\n",
      "463 interests\n",
      "927 international\n",
      "788 interview\n",
      "1036 investigation\n",
      "494 involved\n",
      "919 iowa\n",
      "1484 iran\n",
      "1102 iraq\n",
      "363 iraqi\n",
      "1331 isis\n",
      "872 islamic\n",
      "869 isn\n",
      "792 israel\n",
      "329 israeli\n",
      "1307 issue\n",
      "1198 issues\n",
      "460 james\n",
      "418 january\n",
      "569 jeb\n",
      "843 job\n",
      "631 jobs\n",
      "1398 john\n",
      "515 johnson\n",
      "400 judge\n",
      "439 july\n",
      "443 june\n",
      "4927 just\n",
      "1192 justice\n",
      "553 kasich\n",
      "727 key\n",
      "795 killed\n",
      "821 kind\n",
      "403 king\n",
      "369 knew\n",
      "2629 know\n",
      "895 known\n",
      "441 knows\n",
      "374 labor\n",
      "351 lack\n",
      "418 land\n",
      "802 large\n",
      "344 largely\n",
      "342 largest\n",
      "712 late\n",
      "1028 later\n",
      "534 latest\n",
      "2291 law\n",
      "342 lawmakers\n",
      "559 laws\n",
      "957 lead\n",
      "830 leader\n",
      "1086 leaders\n",
      "491 leadership\n",
      "551 leading\n",
      "590 leave\n",
      "860 led\n",
      "1543 left\n",
      "719 legal\n",
      "406 legislation\n",
      "1166 let\n",
      "479 letter\n",
      "727 level\n",
      "619 liberal\n",
      "361 libya\n",
      "1509 life\n",
      "371 light\n",
      "4818 like\n",
      "1412 likely\n",
      "859 line\n",
      "605 list\n",
      "1167 little\n",
      "686 live\n",
      "718 lives\n",
      "502 living\n",
      "1027 ll\n",
      "722 local\n",
      "1957 long\n",
      "556 longer\n",
      "1298 look\n",
      "691 looking\n",
      "381 lose\n",
      "646 lost\n",
      "1173 lot\n",
      "559 love\n",
      "540 low\n",
      "408 lower\n",
      "365 main\n",
      "438 mainstream\n",
      "1130 major\n",
      "947 majority\n",
      "2936 make\n",
      "697 makes\n",
      "1075 making\n",
      "1383 man\n",
      "595 march\n",
      "399 marco\n",
      "332 mark\n",
      "621 market\n",
      "569 marriage\n",
      "477 mass\n",
      "428 massive\n",
      "931 matter\n",
      "443 maybe\n",
      "606 mean\n",
      "927 means\n",
      "2493 media\n",
      "365 medical\n",
      "364 meet\n",
      "626 meeting\n",
      "527 member\n",
      "1121 members\n",
      "880 men\n",
      "632 message\n",
      "352 met\n",
      "403 mexico\n",
      "398 michael\n",
      "946 middle\n",
      "1973 military\n",
      "1655 million\n",
      "703 millions\n",
      "521 mind\n",
      "638 minister\n",
      "344 modern\n",
      "649 moment\n",
      "869 monday\n",
      "1655 money\n",
      "969 month\n",
      "1048 months\n",
      "572 morning\n",
      "341 mother\n",
      "645 movement\n",
      "868 mr\n",
      "522 muslim\n",
      "484 muslims\n",
      "1258 nation\n",
      "2462 national\n",
      "566 nations\n",
      "445 nato\n",
      "376 natural\n",
      "351 nature\n",
      "581 near\n",
      "738 nearly\n",
      "329 necessary\n",
      "1693 need\n",
      "514 needed\n",
      "691 needs\n",
      "371 netanyahu\n",
      "445 network\n",
      "6243 new\n",
      "2959 news\n",
      "1192 night\n",
      "920 nomination\n",
      "1198 nominee\n",
      "646 non\n",
      "908 north\n",
      "366 note\n",
      "451 noted\n",
      "1209 november\n",
      "1166 nuclear\n",
      "1194 number\n",
      "565 numbers\n",
      "5750 obama\n",
      "487 obamacare\n",
      "1213 october\n",
      "333 offered\n",
      "1324 office\n",
      "387 officer\n",
      "542 officers\n",
      "976 official\n",
      "1439 officials\n",
      "585 ohio\n",
      "582 oil\n",
      "1156 old\n",
      "350 ones\n",
      "350 online\n",
      "725 open\n",
      "384 operation\n",
      "366 operations\n",
      "430 opinion\n",
      "420 opportunity\n",
      "438 opposition\n",
      "1035 order\n",
      "476 organization\n",
      "664 outside\n",
      "342 page\n",
      "516 paid\n",
      "364 paper\n",
      "341 parents\n",
      "491 paris\n",
      "378 particular\n",
      "536 particularly\n",
      "612 parties\n",
      "3878 party\n",
      "379 pass\n",
      "407 passed\n",
      "1293 past\n",
      "350 path\n",
      "967 paul\n",
      "870 pay\n",
      "477 peace\n",
      "8185 people\n",
      "2867 percent\n",
      "361 period\n",
      "956 person\n",
      "772 personal\n",
      "388 phone\n",
      "1195 place\n",
      "905 plan\n",
      "430 planned\n",
      "578 plans\n",
      "605 play\n",
      "506 podesta\n",
      "1520 point\n",
      "811 points\n",
      "2062 police\n",
      "623 policies\n",
      "2060 policy\n",
      "3544 political\n",
      "501 politicians\n",
      "1196 politics\n",
      "920 poll\n",
      "395 polling\n",
      "982 polls\n",
      "344 poor\n",
      "390 popular\n",
      "518 population\n",
      "654 position\n",
      "365 positions\n",
      "918 possible\n",
      "1349 post\n",
      "405 posted\n",
      "705 potential\n",
      "1702 power\n",
      "486 powerful\n",
      "396 present\n",
      "643 presidency\n",
      "6077 president\n",
      "2884 presidential\n",
      "1208 press\n",
      "356 pressure\n",
      "400 pretty\n",
      "394 previous\n",
      "1217 primary\n",
      "432 prime\n",
      "337 prison\n",
      "988 private\n",
      "454 pro\n",
      "726 probably\n",
      "929 problem\n",
      "574 problems\n",
      "1011 process\n",
      "344 professor\n",
      "863 program\n",
      "427 project\n",
      "469 protect\n",
      "466 provide\n",
      "2293 public\n",
      "533 published\n",
      "343 push\n",
      "627 putin\n",
      "1176 question\n",
      "796 questions\n",
      "403 quickly\n",
      "451 quite\n",
      "1584 race\n",
      "350 racial\n",
      "380 racist\n",
      "347 radio\n",
      "391 raise\n",
      "413 raised\n",
      "547 rally\n",
      "491 rate\n",
      "370 reach\n",
      "632 read\n",
      "410 ready\n",
      "384 reagan\n",
      "1326 real\n",
      "590 reality\n",
      "1484 really\n",
      "822 reason\n",
      "341 reasons\n",
      "492 received\n",
      "1325 recent\n",
      "632 recently\n",
      "658 record\n",
      "355 records\n",
      "369 red\n",
      "511 reform\n",
      "368 regime\n",
      "420 region\n",
      "460 related\n",
      "472 relations\n",
      "356 relationship\n",
      "427 release\n",
      "732 released\n",
      "667 religious\n",
      "430 remain\n",
      "445 remains\n",
      "358 remember\n",
      "417 rep\n",
      "1337 report\n",
      "1002 reported\n",
      "539 reporters\n",
      "728 reports\n",
      "3910 republican\n",
      "2421 republicans\n",
      "630 research\n",
      "329 respect\n",
      "578 response\n",
      "493 rest\n",
      "632 result\n",
      "654 results\n",
      "366 return\n",
      "336 revealed\n",
      "385 review\n",
      "2784 right\n",
      "1228 rights\n",
      "476 rise\n",
      "537 risk\n",
      "709 role\n",
      "643 romney\n",
      "385 room\n",
      "1381 rubio\n",
      "437 rule\n",
      "623 rules\n",
      "386 ruling\n",
      "1163 run\n",
      "903 running\n",
      "1962 russia\n",
      "1331 russian\n",
      "661 ryan\n",
      "403 safe\n",
      "14106 said\n",
      "2871 sanders\n",
      "560 saturday\n",
      "623 saudi\n",
      "490 saw\n",
      "2676 say\n",
      "1410 saying\n",
      "2071 says\n",
      "803 school\n",
      "381 science\n",
      "1097 second\n",
      "520 secret\n",
      "1376 secretary\n",
      "1840 security\n",
      "996 seen\n",
      "675 self\n",
      "1098 sen\n",
      "1393 senate\n",
      "838 senator\n",
      "603 senior\n",
      "600 sense\n",
      "611 sent\n",
      "419 september\n",
      "399 series\n",
      "357 serve\n",
      "333 served\n",
      "466 server\n",
      "639 service\n",
      "352 services\n",
      "919 set\n",
      "351 seven\n",
      "512 sex\n",
      "334 sexual\n",
      "787 share\n",
      "411 shooting\n",
      "538 short\n",
      "462 shot\n",
      "488 showed\n",
      "392 showing\n",
      "612 shows\n",
      "559 sign\n",
      "472 significant\n",
      "582 similar\n",
      "703 simply\n",
      "545 single\n",
      "393 site\n",
      "457 situation\n",
      "768 small\n",
      "1164 social\n",
      "567 society\n",
      "370 son\n",
      "531 soon\n",
      "769 source\n",
      "433 sources\n",
      "936 south\n",
      "446 speak\n",
      "541 speaker\n",
      "556 speaking\n",
      "555 special\n",
      "1085 speech\n",
      "527 spending\n",
      "490 spent\n",
      "378 spoke\n",
      "386 spokesman\n",
      "495 staff\n",
      "565 stage\n",
      "577 stand\n",
      "515 standing\n",
      "398 star\n",
      "729 start\n",
      "541 started\n",
      "6113 state\n",
      "1027 statement\n",
      "4245 states\n",
      "360 status\n",
      "401 stay\n",
      "437 step\n",
      "883 stop\n",
      "343 stories\n",
      "924 story\n",
      "526 strategy\n",
      "868 street\n",
      "749 strong\n",
      "455 students\n",
      "519 study\n",
      "780 sunday\n",
      "433 super\n",
      "2440 support\n",
      "1206 supporters\n",
      "405 supporting\n",
      "738 supreme\n",
      "791 sure\n",
      "1511 syria\n",
      "697 syrian\n",
      "755 taken\n",
      "423 takes\n",
      "849 taking\n",
      "761 talk\n",
      "640 talking\n",
      "359 talks\n",
      "329 target\n",
      "1000 tax\n",
      "344 taxes\n",
      "690 team\n",
      "560 ted\n",
      "671 tell\n",
      "340 telling\n",
      "754 term\n",
      "436 terms\n",
      "411 terror\n",
      "568 terrorism\n",
      "539 terrorist\n",
      "438 terrorists\n",
      "333 test\n",
      "813 texas\n",
      "1066 thing\n",
      "1447 things\n",
      "2815 think\n",
      "339 thinking\n",
      "691 thought\n",
      "593 thousands\n",
      "697 threat\n",
      "1016 thursday\n",
      "4644 time\n",
      "1621 times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472 today\n",
      "2673 told\n",
      "1016 took\n",
      "437 total\n",
      "394 town\n",
      "792 trade\n",
      "543 tried\n",
      "436 troops\n",
      "813 true\n",
      "14641 trump\n",
      "380 trust\n",
      "604 truth\n",
      "605 try\n",
      "973 trying\n",
      "1226 tuesday\n",
      "349 turkey\n",
      "671 turn\n",
      "571 turned\n",
      "430 tv\n",
      "767 twitter\n",
      "374 ukraine\n",
      "592 understand\n",
      "560 union\n",
      "2534 united\n",
      "1026 university\n",
      "402 usa\n",
      "1683 use\n",
      "1450 used\n",
      "783 using\n",
      "333 various\n",
      "1639 ve\n",
      "372 veterans\n",
      "578 vice\n",
      "731 victory\n",
      "990 video\n",
      "575 view\n",
      "439 views\n",
      "723 violence\n",
      "360 virginia\n",
      "2392 vote\n",
      "421 voted\n",
      "460 voter\n",
      "2526 voters\n",
      "795 votes\n",
      "1056 voting\n",
      "1001 wall\n",
      "2076 want\n",
      "573 wanted\n",
      "683 wants\n",
      "2822 war\n",
      "370 wars\n",
      "1978 washington\n",
      "553 wasn\n",
      "557 watch\n",
      "718 water\n",
      "2923 way\n",
      "518 ways\n",
      "648 weapons\n",
      "331 website\n",
      "968 wednesday\n",
      "1853 week\n",
      "697 weeks\n",
      "781 went\n",
      "682 west\n",
      "632 western\n",
      "3089 white\n",
      "462 wife\n",
      "587 wikileaks\n",
      "364 willing\n",
      "1444 win\n",
      "371 wing\n",
      "524 winning\n",
      "342 wins\n",
      "740 woman\n",
      "1652 women\n",
      "1381 won\n",
      "427 word\n",
      "691 words\n",
      "1803 work\n",
      "516 worked\n",
      "577 workers\n",
      "1123 working\n",
      "3328 world\n",
      "414 worse\n",
      "358 worth\n",
      "402 wouldn\n",
      "561 wrong\n",
      "897 wrote\n",
      "3384 year\n",
      "3452 years\n",
      "512 yes\n",
      "1506 york\n",
      "743 young\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Подсчитаем количество слов в наших данных\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# Выведем для каждого слова его количество\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем построить логистическую модель\n",
    "В данный момент у нас есть численные значения признаков из обучающей выборки из Bag of Words и оригинальные лейблы новости, поэтому займемся обучением с учителем. Мы будем использовать классификатор логистическую регрессию. Этот алгоритм включен в scikit-learn. Я не буду вдавать в подробности работы алгоритма, здесь мне важно показать, что такая структура данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Учим алгоритм предсказывает класс новости - fake или real\n",
    "reg.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем предсказание\n",
    "Осталось только запустить подготовленный алгоритм на нашей тестовой выборке и проверить результаты предсказания.\n",
    "\n",
    "Стоит отметить, что для тестовой выборки мы уже используем метод \"transform\", а не \"fit_transform\" в Bag of Words.\n",
    "\n",
    "Но для начала преобразуем нашу тестовую выборку в bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# очищаем тестовую подвыбоку\n",
    "\n",
    "# Конвертируем получившиеся bag of words в массив Numpy\n",
    "test_data_features = vectorizer.transform(X_test['text'])\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Делаем предсказание логистической регрессией\n",
    "result = reg.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим насколько точны были предсказания. Accuracy_score - находит долю правильно предсказанных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866571018651363"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86% - Отличный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "В больших текстах, некоторые слова могут встречаться очень часто (например “the”, “a”, “is” в аглийском) при этом неся мало информации об истинном содержании текста. Поэтому мы можем добавить веса словами.\n",
    "\n",
    "Tf-idf означает \"frequency-inverse document frequency\" (частотно-обратная частота документа) и представляет собой статистическую меру, используемую для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции. \n",
    "\n",
    "Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.\n",
    "\n",
    "\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}} ,\n",
    "\n",
    "\n",
    "https://ru.wikipedia.org/wiki/TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf слова \"заяц\" в первом документе 0.1\n",
      "tf слова \"заяц\" во втором документе 0.0\n",
      "tf слова \"заяц\" во третьем документе 0.18181818181818182\n",
      "idf для слова заяц 0.17609125905568124\n",
      "tf*idf слова \"заяц\" в первом документе 0.017609125905568124\n",
      "tf*idf слова \"заяц\" во втором документе 0.0\n",
      "tf*idf слова \"заяц\" во третьем документе 0.032016592555578406\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "example1 = 'в этом документе десять слов и одно из них заяц'.split()\n",
    "example2 = 'а в этом такого слова нет'.split()\n",
    "example3 = 'а здесь слова заяц снова есть есть заяц в этом документе'.split()\n",
    "\n",
    "print('tf слова \"заяц\" в первом документе', example1.count('заяц')/len(example1))\n",
    "print('tf слова \"заяц\" во втором документе', example2.count('заяц')/len(example2))\n",
    "print('tf слова \"заяц\" во третьем документе', example3.count('заяц')/len(example3))\n",
    "\n",
    "print('idf для слова заяц', math.log10(3/2))\n",
    "\n",
    "print('tf*idf слова \"заяц\" в первом документе', example1.count('заяц')/len(example1) * math.log10(3/2))\n",
    "print('tf*idf слова \"заяц\" во втором документе', example2.count('заяц')/len(example2) * math.log10(3/2))\n",
    "print('tf*idf слова \"заяц\" во третьем документе', example3.count('заяц')/len(example3) * math.log10(3/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Tfidvectorizer для конвертации данных в веса tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Инициализируем объект TfidfVectorizer: tfidf_vectorizer\n",
    "# А также ограничили выдачу слов, которые мало встречаются в текстах, \n",
    "# использовав параметр max_df\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.7, min_df = 0.2) \n",
    "# Ограничиваем минимальный и максимальный порог встречаемости слова в текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем на тех же тестовых данных, что и CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренируем классификатор\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train['text'])\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что получили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '2016', 'about', 'according', 'after']\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.14373782 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.24683315 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.06972137 ... 0.         0.16109609 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Выведем 10 первых признаков\n",
    "print(tfidf_vectorizer.get_feature_names()[:5])\n",
    "\n",
    "# Выведем первые 5 векторов обучающей выборки\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.get_feature_names()) # всего слов, удовлетворяющим нашим порогам было 212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, повлияет ли изменение метрики на качество алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493543758967002"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Конвертируем получившиеся bag of words в массив Numpy\n",
    "\n",
    "reg.fit(tfidf_train, y_train)\n",
    "# Делаем предсказание логистической регрессией\n",
    "result = reg.predict(tfidf_test)\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного ухудшилось. Тут на самом деле играет роль то, что с данными в таком виде лучше работают другие алгоритмы, но это уже за пределами нашего курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Для более наглядного примера, мы можем построить Confusion Matrix. Диагональные элементы матрицы указывают на количество верных попаданий для данного класса (Fake или Real в нашем случае). Остальные элементы в строке показывают сколько новостей ушли в другой класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[866, 142],\n",
       "       [173, 910]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "142 фейковых новостей были некорректно отмечены как правдивые. И 173 реальных новостей были отмечены как фейковые.\n",
    "\n",
    "Эта история важна, когда цена ошибки на одном из классов более высока. Например, наша алгоритм чаще называл фейковые новости реальными, чем наоборот. Если нам важно не назвать реальную новость фейковой, то мы ошиблись только 173 раз. А вот если наоборот, то цена ошибки выше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
